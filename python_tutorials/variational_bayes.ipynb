{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayesian Inference\n",
    "\n",
    "In this notebook, we will review the variational Bayes process, beginning with a technical introduction to the formalism and derivation, followed by a python implementation. \n",
    "The material covered here references Blei et al., 2018, Varitional Inference: A Review for Statisticians and Chappel et al., 2016. The FMRIB Variational Bayes Tutorial. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Overview\n",
    "\n",
    "### Problem Statement \n",
    "\n",
    "Similarly to problems addressed by sampling methods, the goal of variational inference (VI) is to approximate parameter distributions from data, specifically in cases where an analytical treatment is intractable. \n",
    "\n",
    "Consider the following example (from Blei et al., 2018): \n",
    "\n",
    "For latent variabels $\\mathbf{z} = z_{1:m}$ and observations $\\mathbf{x} = x_{1:n}$, the posterior conditional density is given by : \n",
    "\n",
    "$$ p(\\mathbf{z} \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid \\mathbf{z}) * p(\\mathbf{z})}{p(\\mathbf{x})} $$\n",
    "\n",
    "The denominator, ${p(\\mathbf{x})}$, whose value is needed to compute the posterior, is calculated by: \n",
    "\n",
    "$$ {p(\\mathbf{x})} = \\int p(\\mathbf{z} , \\mathbf{x}) dz $$\n",
    "\n",
    "\n",
    "This integral is often intractable or too computationally expensive to be feasible. For similar reasons (notably the number of latent variables)sampling methods are slow to converge.\n",
    "\n",
    "VI aims to circumvent the large time complexity by approaching the problem through optimisation. \n",
    "\n",
    "The process begins by positing a contrived _approximate_ density, $\\mathfrak{D}$ of latent variables $\\mathbf{z}$. Then, using this density can find a set of valuers for $q(\\mathbf{z}) \\in \\mathfrak{D}$ whose values maximise the Kullback-Liebler divergence between the approximate density and the true posterior. \n",
    "\n",
    "$$ q^{*}(\\mathbf{z}) = \\mathop{argmin}_{q(\\mathbf{z}) \\in \\mathfrak{D}} \\mathrm{KL}(q(\\mathbf{z}) \\mid \\mid p(\\mathbf{z} \\mid \\mathbf{x}))$$\n",
    "\n",
    "\n",
    "### VI Computation \n",
    "\n",
    "The ELBO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}